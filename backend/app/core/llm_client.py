"""
LLM å®¢æˆ·ç«¯æ¨¡å—
å°è£…è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹çš„é€»è¾‘,æ”¯æŒ mock æ¨¡å¼ã€OpenAI API å’Œå¼€æºæ–¹æ¡ˆï¼ˆOllamaï¼‰
"""

# ç¦ç”¨ huggingface_hub çš„è¿›åº¦æ¡ä»¥é¿å… tqdm å…¼å®¹æ€§é—®é¢˜
import os
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

import asyncio
import logging
from typing import List, Dict
from app.core.config import settings
from app.models.chat import Message

logger = logging.getLogger(__name__)


class LLMClient:
    """LLM å®¢æˆ·ç«¯ç±»"""

    def __init__(self):
        self.use_mock = settings.USE_MOCK_LLM
        self.use_opensource = settings.USE_OPENSOURCE

    async def call_llm(self, messages: List[Message]) -> str:
        """
        è°ƒç”¨ LLM è·å–å›å¤

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨,åŒ…å« system promptã€å†å²å¯¹è¯å’Œå½“å‰ç”¨æˆ·è¾“å…¥

        Returns:
            AI çš„å›å¤æ–‡æœ¬
        """
        if self.use_mock:
            return await self._mock_llm_response(messages)
        elif self.use_opensource:
            return await self._call_ollama(messages)
        else:
            return await self._call_openai(messages)

    async def _mock_llm_response(self, messages: List[Message]) -> str:
        """
        Mock LLM å“åº”(ç”¨äºå¼€å‘æµ‹è¯•)
        æ ¹æ® system prompt çš„å†…å®¹(å³æ¨¡å¼)è¿”å›ä¸åŒçš„ç¤ºä¾‹å›å¤
        """
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.5)

        # è·å– system prompt æ¥åˆ¤æ–­å½“å‰æ¨¡å¼
        system_prompt = ""
        user_message = ""

        for msg in messages:
            if msg.role == "system":
                system_prompt = msg.content
            elif msg.role == "user":
                user_message = msg.content

        # æ ¹æ® system prompt å…³é”®è¯åˆ¤æ–­æ¨¡å¼
        if "æ¼”è®²æ•™ç»ƒ" in system_prompt:
            return self._mock_ppt_response(user_message)
        elif "é¢è¯•å®˜" in system_prompt or "é¢è¯•æ•™ç»ƒ" in system_prompt:
            return self._mock_interview_response(user_message)
        elif "è‡ªæˆ‘ä»‹ç»" in system_prompt:
            return self._mock_self_intro_response(user_message)
        else:
            return "ä½ å¥½!æˆ‘æ˜¯ä½ çš„ AI å£è¯­æ•™ç»ƒ,å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚"

    def _mock_ppt_response(self, user_message: str) -> str:
        """Mock PPT æ¨¡å¼çš„å›å¤"""
        return f"""**ç†è§£ç¡®è®¤:**
æˆ‘ç†è§£ä½ æƒ³è¡¨è¾¾çš„æ˜¯:{user_message[:50]}{'...' if len(user_message) > 50 else ''}

**æ”¹è¿›å»ºè®®:**
1. **å¼€åœºæ›´æœ‰åŠ›**: å¯ä»¥å…ˆæŠ›å‡ºä¸€ä¸ªé—®é¢˜æˆ–æ•°æ®,å¸å¼•å¬ä¼—æ³¨æ„åŠ›
2. **ç»“æ„ä¼˜åŒ–**: å»ºè®®æ˜ç¡®è¯´å‡º"æˆ‘ä»Šå¤©è¦è®²ä¸‰ä¸ªè¦ç‚¹",è®©å¬ä¼—æœ‰é¢„æœŸ
3. **è¯­è¨€ç²¾ç‚¼**: æœ‰äº›è¡¨è¿°å¯ä»¥æ›´ç®€æ´,é¿å…å†—ä½™

**ç¤ºèŒƒè¯´æ³•:**
"å¤§å®¶å¥½!ä½ ä»¬çŸ¥é“å—,[æ ¸å¿ƒæ•°æ®/é—®é¢˜]ã€‚ä»Šå¤©æˆ‘æƒ³ç”¨ 5 åˆ†é’Ÿæ—¶é—´,å’Œå¤§å®¶åˆ†äº«æˆ‘ä»¬å›¢é˜Ÿåœ¨è¿™ä¸ªé¡¹ç›®ä¸Šçš„ä¸‰ä¸ªå…³é”®è¿›å±•:ç¬¬ä¸€...,ç¬¬äºŒ...,ç¬¬ä¸‰..."

ç»§ç»­ç»ƒä¹ å§!ä½ å¯ä»¥è¯•ç€æŒ‰è¿™ä¸ªç»“æ„å†è¯´ä¸€éã€‚"""

    def _mock_interview_response(self, user_message: str) -> str:
        """Mock é¢è¯•æ¨¡å¼çš„å›å¤"""
        # ç®€å•åˆ¤æ–­:å¦‚æœç”¨æˆ·æ¶ˆæ¯å¾ˆçŸ­,å¯èƒ½æ˜¯åˆšå¼€å§‹,æˆ‘ä»¬å…ˆæé—®
        if len(user_message) < 20:
            return """å¾ˆé«˜å…´è§åˆ°ä½ !è®©æˆ‘ä»¬å¼€å§‹æ¨¡æ‹Ÿé¢è¯•å§ã€‚

**ç¬¬ä¸€ä¸ªé—®é¢˜:**
è¯·ç”¨ 1-2 åˆ†é’Ÿæ—¶é—´åšä¸€ä¸ªè‡ªæˆ‘ä»‹ç»,é‡ç‚¹è®²è®²ä½ çš„èƒŒæ™¯ã€æŠ€èƒ½å’Œä¸ºä»€ä¹ˆå¯¹è¿™ä¸ªèŒä½æ„Ÿå…´è¶£ã€‚

(å›ç­”åæˆ‘ä¼šç»™ä½ è¯¦ç»†çš„åé¦ˆ)"""
        else:
            # å¦‚æœç”¨æˆ·å·²ç»å›ç­”äº†é—®é¢˜,æˆ‘ä»¬ç»™ç‚¹è¯„
            return f"""**ç»“æ„åˆ†æ:**
ä½ çš„å›ç­”æ¶µç›–äº†åŸºæœ¬ä¿¡æ¯,ä½†ç»“æ„ä¸Šå¯ä»¥æ›´æ¸…æ™°ã€‚å»ºè®®ä½¿ç”¨"ç°åœ¨-è¿‡å»-æœªæ¥"çš„æ¡†æ¶ã€‚

**æ”¹è¿›å»ºè®®:**
- å¼€å¤´å¯ä»¥ç”¨ä¸€å¥è¯æ€»ç»“ä½ çš„å®šä½,æ¯”å¦‚"æˆ‘æ˜¯ä¸€åä¸“æ³¨äº XX çš„ XX"
- è¿‡å»çš„ç»å†è¦æœ‰ 1-2 ä¸ªå…·ä½“ä¾‹å­æ”¯æ’‘
- è¯´æ˜ä½ ä¸ºä»€ä¹ˆé€‚åˆè¿™ä¸ªå²—ä½(æœªæ¥å±•æœ›)

**ç¤ºèŒƒå›ç­”:**
"æˆ‘æ˜¯ä¸€åæœ‰ X å¹´ç»éªŒçš„[èŒä½],ä¸“æ³¨äº[é¢†åŸŸ]ã€‚åœ¨ä¸Šä¸€ä»½å·¥ä½œä¸­,æˆ‘ä¸»å¯¼äº†[å…·ä½“é¡¹ç›®],å®ç°äº†[å…·ä½“æˆæœ]ã€‚æˆ‘å¯¹è´µå…¬å¸çš„[XX]éå¸¸æ„Ÿå…´è¶£,ç›¸ä¿¡æˆ‘çš„[æŠ€èƒ½]å¯ä»¥ä¸ºå›¢é˜Ÿå¸¦æ¥ä»·å€¼..."

**è¿½é—®:**
ä½ åˆšæ‰æåˆ°äº†[æŸä¸ªé¡¹ç›®/ç»å†],èƒ½å…·ä½“è¯´è¯´ä½ åœ¨å…¶ä¸­é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆå—?"""

    def _mock_self_intro_response(self, user_message: str) -> str:
        """Mock è‡ªæˆ‘ä»‹ç»æ¨¡å¼çš„å›å¤ - ç¤ºèŒƒæ•™å­¦æ³•"""
        return """**å¿«é€Ÿè¯Šæ–­ï¼š**
ä½ çš„è‡ªæˆ‘ä»‹ç»åŒ…å«äº†åŸºæœ¬ä¿¡æ¯ï¼Œä½†è¡¨è¾¾æ¯”è¾ƒå¹³æ·¡ï¼Œç¼ºå°‘å…·ä½“äº‹ä¾‹å’Œè®°å¿†ç‚¹ã€‚

**ç›´æ¥ç¤ºèŒƒ - ä½ å¯ä»¥è¿™æ ·è¯´ï¼š**

"å¤§å®¶å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œç›®å‰æ˜¯é«˜äºŒçš„å­¦ç”Ÿï¼Œå¯¹ç¼–ç¨‹å’Œäººå·¥æ™ºèƒ½ç‰¹åˆ«ç€è¿·ã€‚å»å¹´æˆ‘è‡ªå­¦ Python åï¼Œåšäº†ä¸€ä¸ªå¸®åŠ©åŒå­¦ä»¬æŸ¥è¯¢è¯¾è¡¨çš„å°ç¨‹åºï¼Œç°åœ¨å…¨æ ¡æœ‰ 200 å¤šäººåœ¨ç”¨ã€‚æˆ‘è¿˜æ‹…ä»»å­¦ç”Ÿä¼šæŠ€æœ¯éƒ¨é•¿ï¼Œè´Ÿè´£ç»´æŠ¤å­¦æ ¡å®˜ç½‘ã€‚å¹³æ—¶æˆ‘å–œæ¬¢åœ¨å¼€æºç¤¾åŒºå­¦ä¹ ï¼Œä¹Ÿå¸¸å¸¸å’ŒåŒå­¦åˆ†äº«æœ‰è¶£çš„æŠ€æœ¯ã€‚æœªæ¥æˆ‘å¸Œæœ›èƒ½è¿›å…¥è®¡ç®—æœºä¸“ä¸šæ·±é€ ï¼Œç”¨æŠ€æœ¯è®©ç”Ÿæ´»æ›´ä¾¿æ·ã€‚å¾ˆé«˜å…´è®¤è¯†å¤§å®¶ï¼"

**ä¸ºä»€ä¹ˆè¿™æ ·è¯´æ›´å¥½ï¼š**
1. **æœ‰å…·ä½“æˆæœ** - "200å¤šäººåœ¨ç”¨"æ¯”"åšè¿‡å°ç¨‹åº"æ›´æœ‰è¯´æœåŠ›
2. **çªå‡ºè¡ŒåŠ¨åŠ›** - å±•ç¤ºäº†è‡ªå­¦èƒ½åŠ›å’Œå®è·µç»éªŒ
3. **æœ‰æ¸…æ™°ç›®æ ‡** - è¯´æ˜äº†æœªæ¥æ–¹å‘ï¼Œæ˜¾å¾—æœ‰è§„åˆ’
4. **æ”¶å°¾å‹å¥½** - è‡ªç„¶åœ°æ‰“å¼€è¯é¢˜ï¼Œä¾¿äºç»§ç»­äº¤æµ

**ï¼ˆå¦‚æœæ˜¯ç¤¾äº¤åœºåˆï¼Œå¯ä»¥è¿™æ ·è¯´ï¼‰ï¼š**
"å—¨å¤§å®¶å¥½ï¼æˆ‘æ˜¯å°æ˜ï¼Œä¸€ä¸ªçƒ­çˆ±ç¼–ç¨‹çš„é«˜ä¸­ç”Ÿã€‚æœ€è¿‘åœ¨æ£é¼“ä¸€ä¸ªè¯¾è¡¨æŸ¥è¯¢å°å·¥å…·ï¼Œæ²¡æƒ³åˆ°æŒºå—æ¬¢è¿çš„ï¼Œç°åœ¨å…¨æ ¡ 200 å¤šäººéƒ½åœ¨ç”¨ã€‚å¹³æ—¶é™¤äº†å†™ä»£ç ï¼Œæˆ‘ä¹Ÿå–œæ¬¢é€›æŠ€æœ¯ç¤¾åŒºï¼Œç»å¸¸èƒ½å‘ç°å¾ˆé…·çš„é¡¹ç›®ã€‚å¸Œæœ›èƒ½å’Œå¤§å®¶å¤šäº¤æµï¼Œä¸€èµ·å­¦ä¹ è¿›æ­¥ï¼"

---

ğŸ’¡ **ç»ƒä¹ å»ºè®®ï¼š** è¯•ç€ç”¨è¿™ä¸ªç»“æ„å†è¯´ä¸€éï¼Œè®°å¾—åŠ å…¥ä½ è‡ªå·±çš„å…·ä½“äº‹ä¾‹ï¼"""

    async def _call_ollama(self, messages: List[Message]) -> str:
        """
        è°ƒç”¨æœ¬åœ° Ollama APIï¼ˆå¼€æºæ–¹æ¡ˆï¼‰
        """
        import httpx

        url = f"{settings.OLLAMA_BASE_URL}/api/chat"

        payload = {
            "model": settings.OLLAMA_MODEL,
            "messages": [{"role": m.role, "content": m.content} for m in messages],
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 2500  # å¢åŠ æœ€å¤§ token æ•°ï¼Œé¿å…é¢è¯•è¯„ä»·è¢«æˆªæ–­
            }
        }

        try:
            # trust_env=False ç¦ç”¨ä»£ç†,ç¡®ä¿ localhost è¿æ¥æ­£å¸¸
            async with httpx.AsyncClient(timeout=120.0, trust_env=False) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                result = response.json()
                return result["message"]["content"]
        except httpx.ConnectError:
            logger.error("æ— æ³•è¿æ¥åˆ° Ollamaï¼Œè¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ")
            raise ConnectionError("æ— æ³•è¿æ¥åˆ° Ollamaã€‚è¯·è¿è¡Œ: ollama serve")
        except Exception as e:
            logger.error(f"Ollama è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_openai(self, messages: List[Message]) -> str:
        """
        è°ƒç”¨ OpenAI APIï¼ˆä»˜è´¹æ–¹æ¡ˆï¼‰
        """
        from openai import AsyncOpenAI

        client = AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL
        )

        response = await client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=[{"role": m.role, "content": m.content} for m in messages],
            temperature=0.7,
            max_tokens=1500
        )

        return response.choices[0].message.content


# ============ ASR (è¯­éŸ³è½¬æ–‡å­—) ============

async def transcribe_audio(audio_content: bytes, filename: str = "audio.webm") -> str:
    """
    å°†è¯­éŸ³æ–‡ä»¶è½¬ä¸ºæ–‡å­—

    Args:
        audio_content: éŸ³é¢‘æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
        filename: æ–‡ä»¶åï¼ˆç”¨äºæŒ‡å®šæ–‡ä»¶ç±»å‹ï¼‰

    Returns:
        è¯†åˆ«å‡ºçš„æ–‡å­—
    """
    if settings.USE_MOCK_LLM:
        # Mock æ¨¡å¼ï¼šè¿”å›ç¤ºä¾‹æ–‡æœ¬
        await asyncio.sleep(0.5)
        return """å¤§å®¶å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œç›®å‰æ˜¯ä¸€åé«˜ä¸­äºŒå¹´çº§çš„å­¦ç”Ÿã€‚
æˆ‘å¯¹è®¡ç®—æœºç¼–ç¨‹å’Œäººå·¥æ™ºèƒ½ç‰¹åˆ«æ„Ÿå…´è¶£ï¼Œå¹³æ—¶å–œæ¬¢è‡ªå­¦ä¸€äº›ç¼–ç¨‹è¯¾ç¨‹ã€‚
åœ¨å­¦æ ¡é‡Œï¼Œæˆ‘æ‹…ä»»å­¦ç”Ÿä¼šçš„æŠ€æœ¯éƒ¨é•¿ï¼Œè´Ÿè´£ç»´æŠ¤å­¦æ ¡çš„ç½‘ç«™å’Œä¸€äº›å°ç¨‹åºã€‚
æˆ‘çš„æ€§æ ¼æ¯”è¾ƒå¤–å‘ï¼Œå–œæ¬¢å’ŒåŒå­¦ä»¬äº¤æµå­¦ä¹ ç»éªŒã€‚
æœªæ¥æˆ‘å¸Œæœ›èƒ½å¤Ÿè¿›å…¥ä¸€æ‰€å¥½çš„å¤§å­¦ï¼Œç»§ç»­æ·±é€ è®¡ç®—æœºç§‘å­¦ï¼Œ
å¹¶ä¸”èƒ½å¤Ÿç”¨æŠ€æœ¯å¸®åŠ©æ›´å¤šçš„äººè§£å†³å®é™…é—®é¢˜ã€‚è°¢è°¢å¤§å®¶ï¼"""

    elif settings.USE_OPENSOURCE:
        # å¼€æºæ–¹æ¡ˆï¼šä½¿ç”¨ faster-whisper æœ¬åœ°æ¨¡å‹
        return await _transcribe_with_faster_whisper(audio_content, filename)
    else:
        # ä»˜è´¹æ–¹æ¡ˆï¼šè°ƒç”¨ OpenAI Whisper API
        return await _transcribe_with_openai(audio_content, filename)


async def _transcribe_with_faster_whisper(audio_content: bytes, filename: str) -> str:
    """
    ä½¿ç”¨ faster-whisper æœ¬åœ°æ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    import tempfile
    import os

    # å°†éŸ³é¢‘å†…å®¹ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    suffix = "." + filename.split(".")[-1] if "." in filename else ".webm"
    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp_file:
        tmp_file.write(audio_content)
        tmp_path = tmp_file.name

    # å¦‚æœæ˜¯ WebM æ ¼å¼ï¼Œå…ˆè½¬æ¢ä¸º WAVï¼ˆfaster-whisper æ›´å¥½æ”¯æŒï¼‰
    wav_path = None
    if suffix.lower() in ['.webm', '.ogg', '.opus']:
        wav_path = tmp_path.replace(suffix, '.wav')
        try:
            import subprocess
            result = subprocess.run([
                'ffmpeg', '-y',
                '-i', tmp_path,
                '-acodec', 'pcm_s16le',
                '-ar', '16000',
                '-ac', '1',
                wav_path
            ], capture_output=True, text=True)

            if result.returncode == 0:
                logger.info(f"éŸ³é¢‘è½¬æ¢æˆåŠŸ: {suffix} -> .wav")
                audio_path_to_use = wav_path
            else:
                logger.warning(f"ffmpeg è½¬æ¢å¤±è´¥: {result.stderr}, å°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶")
                audio_path_to_use = tmp_path
        except Exception as e:
            logger.warning(f"ffmpeg è½¬æ¢å¼‚å¸¸: {e}, å°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶")
            audio_path_to_use = tmp_path
    else:
        audio_path_to_use = tmp_path

    try:
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ä»£ç ï¼ˆfaster-whisper æ˜¯åŒæ­¥çš„ï¼‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _run_faster_whisper, audio_path_to_use)
        return result
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(tmp_path):
            os.remove(tmp_path)
        if wav_path and os.path.exists(wav_path):
            os.remove(wav_path)


def _run_faster_whisper(audio_path: str) -> str:
    """
    åŒæ­¥è¿è¡Œ faster-whisperï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
    """
    try:
        from faster_whisper import WhisperModel

        # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆ 8GB å†…å­˜
        # local_files_only=True é¿å…é‡æ–°ä¸‹è½½ï¼Œæ¨¡å‹å·²ç»é¢„å…ˆä¸‹è½½å¥½äº†
        model = WhisperModel(
            settings.WHISPER_MODEL,  # tiny / base / small
            device="cpu",
            compute_type="int8",  # ä½¿ç”¨ int8 é‡åŒ–å‡å°‘å†…å­˜
            local_files_only=True  # åªä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹
        )

        segments, info = model.transcribe(
            audio_path,
            language="zh",
            beam_size=5
        )

        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        text = " ".join([segment.text for segment in segments])
        return text.strip()

    except ImportError:
        logger.error("faster-whisper æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install faster-whisper")
        raise ImportError("è¯·å®‰è£… faster-whisper: pip install faster-whisper")
    except Exception as e:
        logger.error(f"faster-whisper è½¬å†™å¤±è´¥: {str(e)}")
        raise


async def _transcribe_with_openai(audio_content: bytes, filename: str) -> str:
    """
    ä½¿ç”¨ OpenAI Whisper API è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼ˆä»˜è´¹ï¼‰
    """
    from openai import AsyncOpenAI
    import io

    client = AsyncOpenAI(
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL
    )

    # å°† bytes è½¬æ¢ä¸ºæ–‡ä»¶å¯¹è±¡
    audio_file = io.BytesIO(audio_content)
    audio_file.name = filename

    transcript = await client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="zh"
    )

    return transcript.text


# ============ TTS (æ–‡å­—è½¬è¯­éŸ³) ============

async def synthesize_speech(text: str) -> bytes:
    """
    å°†æ–‡å­—è½¬ä¸ºè¯­éŸ³

    Args:
        text: è¦è½¬æ¢çš„æ–‡å­—

    Returns:
        éŸ³é¢‘æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼ˆMP3 æ ¼å¼ï¼‰
    """
    if settings.USE_MOCK_LLM:
        # Mock æ¨¡å¼ï¼šè¿”å›ç©ºçš„éŸ³é¢‘æ•°æ®
        return b""

    elif settings.USE_OPENSOURCE:
        # å¼€æºæ–¹æ¡ˆï¼šä½¿ç”¨ edge-ttsï¼ˆå¾®è½¯å…è´¹ TTSï¼‰
        return await _synthesize_with_edge_tts(text)
    else:
        # ä»˜è´¹æ–¹æ¡ˆï¼šè°ƒç”¨ OpenAI TTS API
        return await _synthesize_with_openai(text)


async def _synthesize_with_edge_tts(text: str) -> bytes:
    """
    ä½¿ç”¨ edge-ttsï¼ˆå¾®è½¯å…è´¹ TTSï¼‰ç”Ÿæˆè¯­éŸ³
    """
    try:
        import edge_tts
        import tempfile
        import os

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp_file:
            tmp_path = tmp_file.name

        try:
            # ç”Ÿæˆè¯­éŸ³
            communicate = edge_tts.Communicate(
                text,
                voice=settings.EDGE_TTS_VOICE  # zh-CN-XiaoxiaoNeural
            )
            await communicate.save(tmp_path)

            # è¯»å–éŸ³é¢‘å†…å®¹
            with open(tmp_path, "rb") as f:
                audio_content = f.read()

            return audio_content
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_path):
                os.remove(tmp_path)

    except ImportError:
        logger.error("edge-tts æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install edge-tts")
        raise ImportError("è¯·å®‰è£… edge-tts: pip install edge-tts")
    except Exception as e:
        logger.error(f"edge-tts ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise


async def _synthesize_with_openai(text: str) -> bytes:
    """
    ä½¿ç”¨ OpenAI TTS API ç”Ÿæˆè¯­éŸ³ï¼ˆä»˜è´¹ï¼‰
    """
    from openai import AsyncOpenAI

    client = AsyncOpenAI(
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL
    )

    response = await client.audio.speech.create(
        model="tts-1",
        voice="nova",
        input=text,
        response_format="mp3"
    )

    return response.content


# ============ Vision (å›¾ç‰‡è¯†åˆ«) ============

async def analyze_slide_with_vision(
    slide_image_url: str,
    user_transcript: str,
    slide_number: int,
    slide_text: str = ""
) -> str:
    """
    åˆ†æ PPT å¹»ç¯ç‰‡å’Œç”¨æˆ·è®²è§£

    æ³¨æ„ï¼š8GB å†…å­˜æ— æ³•è¿è¡Œæœ¬åœ° Vision æ¨¡å‹ï¼Œ
    å¼€æºæ¨¡å¼ä¸‹å°†ä½¿ç”¨æ–‡å­—åˆ†ææ›¿ä»£å›¾ç‰‡è¯†åˆ«

    Args:
        slide_image_url: å¹»ç¯ç‰‡å›¾ç‰‡çš„ URL
        user_transcript: ç”¨æˆ·è®²è§£çš„è½¬å†™æ–‡æœ¬
        slide_number: å¹»ç¯ç‰‡ç¼–å·
        slide_text: ä» PPT æå–çš„æ–‡å­—å†…å®¹

    Returns:
        AI çš„åˆ†æå’Œç¤ºèŒƒæ•™å­¦åé¦ˆ
    """
    if settings.USE_MOCK_LLM:
        # Mock æ¨¡å¼
        await asyncio.sleep(1.0)
        return _get_mock_vision_response(slide_number, slide_text, user_transcript)

    elif settings.USE_OPENSOURCE:
        # å¼€æºæ–¹æ¡ˆï¼šä½¿ç”¨ Ollama è¿›è¡Œæ–‡å­—åˆ†æï¼ˆ8GB å†…å­˜æ— æ³•è¿è¡Œ Vision æ¨¡å‹ï¼‰
        return await _analyze_slide_with_ollama(slide_number, slide_text, user_transcript)
    else:
        # ä»˜è´¹æ–¹æ¡ˆï¼šä½¿ç”¨ GPT-4 Vision
        return await _analyze_slide_with_gpt4_vision(
            slide_image_url, user_transcript, slide_number, slide_text
        )


def _get_mock_vision_response(slide_number: int, slide_text: str, user_transcript: str) -> str:
    """Mock Vision å“åº”"""
    return f"""**å¹»ç¯ç‰‡å†…å®¹åˆ†æï¼ˆç¬¬ {slide_number} é¡µï¼‰ï¼š**
æˆ‘çœ‹åˆ°è¿™é¡µ PPT åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š{slide_text[:100] if slide_text else "å›¾è¡¨å’Œæ–‡å­—"}...

**ä½ çš„è®²è§£åˆ†æï¼š**
ä½ æåˆ°äº†ï¼š{user_transcript[:100]}...
æ€»ä½“æ¥è¯´ï¼Œä½ çš„è®²è§£åŸºæœ¬è¦†ç›–äº†å¹»ç¯ç‰‡çš„æ ¸å¿ƒå†…å®¹ï¼Œä½†å¯ä»¥æ›´åŠ æµç•…å’Œæœ‰æ¡ç†ã€‚

**ç›´æ¥ç¤ºèŒƒ - è¿™ä¸€é¡µä½ å¯ä»¥è¿™æ ·è®²ï¼š**

"å¤§å®¶è¯·çœ‹è¿™ä¸€é¡µã€‚{slide_text.split(chr(10))[0] if slide_text else 'è¿™é‡Œå±•ç¤ºçš„æ˜¯æˆ‘ä»¬çš„æ ¸å¿ƒæ•°æ®'}ã€‚ä»å›¾è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬åœ¨è¿‡å»ä¸€å¹´å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚å…·ä½“æ¥è¯´ï¼Œç¬¬ä¸€ï¼Œ[å…³é”®ç‚¹1]ï¼›ç¬¬äºŒï¼Œ[å…³é”®ç‚¹2]ï¼›ç¬¬ä¸‰ï¼Œ[å…³é”®ç‚¹3]ã€‚è¿™äº›æˆæœå¾—ç›Šäºå›¢é˜Ÿçš„å…±åŒåŠªåŠ›å’ŒæŠ€æœ¯åˆ›æ–°ã€‚"

**ä¸ºä»€ä¹ˆè¿™æ ·è®²æ›´å¥½ï¼š**
1. **å¼€åœºæ˜ç¡®** - å¼•å¯¼å¬ä¼—æ³¨æ„åŠ›åˆ°å¹»ç¯ç‰‡
2. **ç»“æ„æ¸…æ™°** - ç”¨"ç¬¬ä¸€ã€ç¬¬äºŒã€ç¬¬ä¸‰"è®©å¬ä¼—å®¹æ˜“è·Ÿä¸Š
3. **æœ‰æ€»ç»“** - æœ€åç‚¹æ˜æ„ä¹‰ï¼Œè€Œä¸æ˜¯ç®€å•å¿µå®Œå°±ç»“æŸ

**å…³äºè¿™é¡µ PPT çš„ä¼˜åŒ–å»ºè®®ï¼š**
âœ… **ä¿ç•™çš„å†…å®¹ï¼š** æ ¸å¿ƒæ•°æ®å’Œå›¾è¡¨å±•ç¤ºå¾ˆæ¸…æ™°
âš ï¸ **å¯ä»¥æ”¹è¿›ï¼š**
  - æ–‡å­—å¯ä»¥ç²¾ç®€ï¼Œåªä¿ç•™å…³é”®è¯
  - å¦‚æœæœ‰å›¾è¡¨ï¼Œå¯ä»¥çªå‡ºæ˜¾ç¤ºæœ€é‡è¦çš„è¶‹åŠ¿
  - æ ‡é¢˜å¯ä»¥æ›´å¸å¼•äººï¼Œæ¯”å¦‚ç”¨é—®å¥æˆ–æ•°æ®
âŒ **å»ºè®®åˆ å‡ï¼š** å¦‚æœæœ‰è¿‡å¤šçš„è§£é‡Šæ€§æ–‡å­—ï¼Œå¯ä»¥åˆ æ‰ï¼Œæ”¹ä¸ºå£å¤´è®²è§£

---
ğŸ’¡ **ç»ƒä¹ å»ºè®®ï¼š** è¯•ç€ç”¨è¿™ä¸ªç»“æ„å†è®²ä¸€éè¿™ä¸€é¡µï¼Œè®°å¾—æ”¾æ…¢è¯­é€Ÿï¼Œç»™å¬ä¼—æ—¶é—´ç†è§£ï¼"""


async def _analyze_slide_with_ollama(
    slide_number: int,
    slide_text: str,
    user_transcript: str
) -> str:
    """
    ä½¿ç”¨ Ollama è¿›è¡Œæ–‡å­—åˆ†æï¼ˆå¼€æºæ–¹æ¡ˆï¼‰
    æ³¨æ„ï¼š8GB å†…å­˜æ— æ³•è¿è¡Œ Vision æ¨¡å‹ï¼Œæ‰€ä»¥åªåˆ†ææå–çš„æ–‡å­—
    """
    import httpx

    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ PPT æ¼”è®²æ•™ç»ƒï¼Œé‡‡ç”¨"ç¤ºèŒƒæ•™å­¦æ³•"æŒ‡å¯¼ç”¨æˆ·ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†æå¹»ç¯ç‰‡çš„æ–‡å­—å†…å®¹
2. å¯¹æ¯”ç”¨æˆ·è®²è§£æ˜¯å¦è¦†ç›–äº†æ ¸å¿ƒå†…å®¹
3. ç›´æ¥ç¤ºèŒƒå¦‚ä½•è®²è§£è¿™ä¸€é¡µ
4. æä¾› PPT ä¼˜åŒ–å»ºè®®

**æ ¸å¿ƒåŸåˆ™ï¼šä¸è¦åªç»™å»ºè®®ï¼Œè¦ç›´æ¥ç¤ºèŒƒæ€ä¹ˆè®²ï¼**"""

    user_message = f"""**å¹»ç¯ç‰‡ç¼–å·ï¼š** ç¬¬ {slide_number} é¡µ

**å¹»ç¯ç‰‡æ–‡å­—å†…å®¹ï¼š**
{slide_text if slide_text else "ï¼ˆæ— æ–‡å­—å†…å®¹ï¼‰"}

**ç”¨æˆ·çš„è®²è§£ï¼š**
{user_transcript}

---

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **åˆ†æå¹»ç¯ç‰‡å†…å®¹**ï¼šä»æ–‡å­—ä¸­è¯†åˆ«å…³é”®ä¿¡æ¯
2. **å¯¹æ¯”åˆ†æ**ï¼š
   - ç”¨æˆ·è®²è§£æ˜¯å¦è¦†ç›–äº†æ ¸å¿ƒå†…å®¹ï¼Ÿ
   - è®²è§£æ˜¯å¦æ¸…æ™°ã€æœ‰æ¡ç†ï¼Ÿ
3. **ç›´æ¥ç¤ºèŒƒ**ï¼šç»™å‡ºè¿™ä¸€é¡µçš„å®Œæ•´ç¤ºèŒƒè®²è§£ï¼ˆ30-60ç§’ï¼‰
4. **PPT ä¼˜åŒ–å»ºè®®**ï¼š
   - âœ… å“ªäº›å†…å®¹åº”è¯¥ä¿ç•™
   - âš ï¸ å“ªäº›å†…å®¹å¯ä»¥æ”¹è¿›
   - âŒ å“ªäº›å†…å®¹åº”è¯¥åˆ å‡

è¯·ç”¨é¼“åŠ±ã€å®ç”¨çš„è¯­æ°”å›å¤ã€‚"""

    url = f"{settings.OLLAMA_BASE_URL}/api/chat"
    payload = {
        "model": settings.OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 2000
        }
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            result = response.json()
            return result["message"]["content"]
    except httpx.ConnectError:
        logger.error("æ— æ³•è¿æ¥åˆ° Ollama")
        raise ConnectionError("æ— æ³•è¿æ¥åˆ° Ollamaã€‚è¯·è¿è¡Œ: ollama serve")
    except Exception as e:
        logger.error(f"Ollama è°ƒç”¨å¤±è´¥: {str(e)}")
        raise


async def generate_slide_demo_script(
    slide_number: int,
    slide_text: str,
    slide_image_path: str = None
) -> str:
    """
    ä¸ºæŒ‡å®šå¹»ç¯ç‰‡ç”Ÿæˆ AI ç¤ºèŒƒè®²è§£è¯æœ¯

    æ³¨æ„ï¼šä¸éœ€è¦ç”¨æˆ·çš„å½•éŸ³ï¼Œç›´æ¥æ ¹æ® PPT å†…å®¹ç”Ÿæˆç¤ºèŒƒè®²è§£

    Args:
        slide_number: å¹»ç¯ç‰‡ç¼–å·
        slide_text: ä» PPT æå–çš„æ–‡å­—å†…å®¹
        slide_image_path: å¹»ç¯ç‰‡å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¼€æºæ–¹æ¡ˆä¸ä½¿ç”¨ï¼‰

    Returns:
        AI ç¤ºèŒƒè®²è§£è¯æœ¯ï¼ˆ30-60ç§’çš„æ¼”è®²ç¨¿ï¼‰
    """
    if settings.USE_MOCK_LLM:
        # Mock æ¨¡å¼
        await asyncio.sleep(0.5)
        return f"å¤§å®¶å¥½ï¼Œè¯·çœ‹ç¬¬ {slide_number} é¡µã€‚{slide_text[:50] if slide_text else 'è¿™é¡µå±•ç¤ºäº†æˆ‘ä»¬çš„æ ¸å¿ƒå†…å®¹'}ã€‚è®©æˆ‘ä¸ºå¤§å®¶è¯¦ç»†è®²è§£ä¸€ä¸‹..."

    elif settings.USE_OPENSOURCE:
        # å¼€æºæ–¹æ¡ˆï¼šä½¿ç”¨ Ollama ç”Ÿæˆç¤ºèŒƒè¯æœ¯
        return await _generate_demo_script_with_ollama(slide_number, slide_text)
    else:
        # ä»˜è´¹æ–¹æ¡ˆï¼šä½¿ç”¨ GPT-4 Vision ç”Ÿæˆç¤ºèŒƒè¯æœ¯
        return await _generate_demo_script_with_gpt4_vision(slide_number, slide_text, slide_image_path)


async def _generate_demo_script_with_ollama(
    slide_number: int,
    slide_text: str
) -> str:
    """
    ä½¿ç”¨ Ollama ç”Ÿæˆç¤ºèŒƒè®²è§£è¯æœ¯ï¼ˆå¼€æºæ–¹æ¡ˆï¼‰
    """
    import httpx

    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ¼”è®²æ•™ç»ƒï¼Œå¸®åŠ©ç”¨æˆ·å­¦ä¹ å¦‚ä½•è®²è§£ PPTã€‚

ä½ çš„ä»»åŠ¡ï¼š
- æ ¹æ®å¹»ç¯ç‰‡çš„æ–‡å­—å†…å®¹ï¼Œç”Ÿæˆä¸€æ®µ30-60ç§’çš„ç¤ºèŒƒè®²è§£è¯æœ¯
- è®²è§£è¦æ¸…æ™°ã€æœ‰æ¡ç†ã€å¼•äººå…¥èƒœ
- ä½¿ç”¨\"å¤§å®¶è¯·çœ‹\"ã€\"ç¬¬ä¸€\"ã€\"ç¬¬äºŒ\"ç­‰è¿‡æ¸¡è¯
- è¯­è¨€è¦å£è¯­åŒ–ï¼ŒåƒçœŸäººåœ¨æ¼”è®²

**è¦æ±‚ï¼šåªè¾“å‡ºç¤ºèŒƒè®²è§£çš„è¯æœ¯ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæˆ–æ ‡é¢˜ã€‚**"""

    user_message = f"""**å¹»ç¯ç‰‡ç¼–å·ï¼š** ç¬¬ {slide_number} é¡µ

**å¹»ç¯ç‰‡æ–‡å­—å†…å®¹ï¼š**
{slide_text if slide_text else "ï¼ˆå›¾è¡¨æˆ–å›¾ç‰‡ï¼Œæ— æ–‡å­—ï¼‰"}

---

è¯·ç”Ÿæˆä¸€æ®µ30-60ç§’çš„ç¤ºèŒƒè®²è§£è¯æœ¯ï¼Œæ¼”ç¤ºå¦‚ä½•è®²è§£è¿™ä¸€é¡µ PPTã€‚
åªè¾“å‡ºè®²è§£è¯æœ¯æœ¬èº«ï¼Œä¸è¦åŠ æ ‡é¢˜æˆ–è§£é‡Šã€‚"""

    url = f"{settings.OLLAMA_BASE_URL}/api/chat"
    payload = {
        "model": settings.OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 500
        }
    }

    try:
        async with httpx.AsyncClient(timeout=60.0, trust_env=False) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            result = response.json()
            demo_script = result["message"]["content"].strip()

            # ç§»é™¤å¯èƒ½çš„æ ‡é¢˜æˆ–æ ¼å¼
            demo_script = demo_script.replace("**ç¤ºèŒƒè®²è§£ï¼š**", "")
            demo_script = demo_script.replace("**è¯æœ¯ï¼š**", "")
            demo_script = demo_script.strip()

            return demo_script
    except Exception as e:
        logger.error(f"ç”Ÿæˆç¤ºèŒƒè¯æœ¯å¤±è´¥: {str(e)}")
        # è¿”å›ç®€å•çš„ç¤ºèŒƒ
        return f"å¤§å®¶å¥½ï¼Œè¯·çœ‹ç¬¬ {slide_number} é¡µã€‚{slide_text[:100] if slide_text else 'è¿™é¡µå±•ç¤ºäº†æˆ‘ä»¬çš„æ ¸å¿ƒå†…å®¹'}ã€‚"


async def _generate_demo_script_with_gpt4_vision(
    slide_number: int,
    slide_text: str,
    slide_image_path: str = None
) -> str:
    """
    ä½¿ç”¨ GPT-4 Vision ç”Ÿæˆç¤ºèŒƒè®²è§£è¯æœ¯ï¼ˆä»˜è´¹æ–¹æ¡ˆï¼‰
    """
    from openai import AsyncOpenAI
    import base64

    client = AsyncOpenAI(
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL
    )

    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œè¯»å–å¹¶ç¼–ç 
    image_data = None
    if slide_image_path and os.path.exists(slide_image_path):
        with open(slide_image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode('utf-8')

    # æ„å»ºæ¶ˆæ¯
    if image_data:
        content = [
            {"type": "text", "text": f"è¿™æ˜¯ç¬¬ {slide_number} é¡µ PPTã€‚è¯·ç”Ÿæˆä¸€æ®µ30-60ç§’çš„ç¤ºèŒƒè®²è§£è¯æœ¯ï¼Œæ¼”ç¤ºå¦‚ä½•è®²è§£è¿™ä¸€é¡µã€‚åªè¾“å‡ºè¯æœ¯æœ¬èº«ï¼Œä¸è¦åŠ æ ‡é¢˜ã€‚"},
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_data}"}}
        ]
    else:
        content = f"è¿™æ˜¯ç¬¬ {slide_number} é¡µ PPTï¼Œå†…å®¹å¦‚ä¸‹ï¼š\n\n{slide_text}\n\nè¯·ç”Ÿæˆä¸€æ®µ30-60ç§’çš„ç¤ºèŒƒè®²è§£è¯æœ¯ã€‚åªè¾“å‡ºè¯æœ¯æœ¬èº«ã€‚"

    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯æ¼”è®²æ•™ç»ƒï¼Œç”Ÿæˆç¤ºèŒƒè®²è§£è¯æœ¯ã€‚"},
            {"role": "user", "content": content}
        ],
        max_tokens=500,
        temperature=0.7
    )

    return response.choices[0].message.content.strip()


async def _analyze_slide_with_gpt4_vision(
    slide_image_url: str,
    user_transcript: str,
    slide_number: int,
    slide_text: str
) -> str:
    """
    ä½¿ç”¨ GPT-4 Vision åˆ†æå¹»ç¯ç‰‡ï¼ˆä»˜è´¹æ–¹æ¡ˆï¼‰
    """
    from openai import AsyncOpenAI
    import base64
    from pathlib import Path

    client = AsyncOpenAI(
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL
    )

    # æ„å»ºå›¾ç‰‡å†…å®¹
    image_content = None
    if slide_image_url.startswith('http'):
        image_content = {
            "type": "image_url",
            "image_url": {"url": slide_image_url}
        }
    else:
        image_path = Path(slide_image_url)
        if image_path.exists():
            with open(image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            image_content = {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{image_data}"
                }
            }

    if not image_content:
        raise ValueError(f"æ— æ³•è¯»å–å¹»ç¯ç‰‡å›¾ç‰‡: {slide_image_url}")

    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ PPT æ¼”è®²æ•™ç»ƒï¼Œé‡‡ç”¨"ç¤ºèŒƒæ•™å­¦æ³•"æŒ‡å¯¼ç”¨æˆ·ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. **åˆ†æå¹»ç¯ç‰‡å†…å®¹** - ä»å›¾ç‰‡ä¸­è¯†åˆ«æ–‡å­—ã€å›¾è¡¨ã€å›¾åƒ
2. **å¯¹æ¯”ç”¨æˆ·è®²è§£** - åˆ¤æ–­ç”¨æˆ·çš„è®²è§£æ˜¯å¦è¦†ç›–äº†å¹»ç¯ç‰‡çš„æ ¸å¿ƒå†…å®¹
3. **ç›´æ¥ç¤ºèŒƒ** - ç»™å‡ºè¿™ä¸€é¡µçš„å®Œæ•´ç¤ºèŒƒè®²è§£è¯æœ¯
4. **PPT ä¼˜åŒ–å»ºè®®** - å»ºè®®è¿™é¡µ PPT åº”è¯¥ä¿ç•™ã€ä¿®æ”¹æˆ–åˆ å‡ä»€ä¹ˆå†…å®¹

**æ ¸å¿ƒåŸåˆ™ï¼šä¸è¦åªç»™å»ºè®®ï¼Œè¦ç›´æ¥ç¤ºèŒƒæ€ä¹ˆè®²ï¼**"""

    user_message = f"""**å¹»ç¯ç‰‡ç¼–å·ï¼š** ç¬¬ {slide_number} é¡µ

**æå–çš„æ–‡å­—å†…å®¹ï¼š**
{slide_text if slide_text else "ï¼ˆæ— æ–‡å­—æå–ï¼‰"}

**ç”¨æˆ·çš„è®²è§£ï¼š**
{user_transcript}

---

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **åˆ†æå¹»ç¯ç‰‡å†…å®¹**ï¼šä»å›¾ç‰‡ä¸­è¯†åˆ«å…³é”®ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€è¦ç‚¹ã€å›¾è¡¨ç­‰ï¼‰
2. **å¯¹æ¯”åˆ†æ**ï¼šç”¨æˆ·è®²è§£æ˜¯å¦è¦†ç›–äº†æ ¸å¿ƒå†…å®¹ï¼Ÿæ˜¯å¦æµç•…ï¼Ÿ
3. **ç›´æ¥ç¤ºèŒƒ**ï¼šç»™å‡ºè¿™ä¸€é¡µçš„å®Œæ•´ç¤ºèŒƒè®²è§£ï¼ˆ30-60ç§’ï¼‰
4. **PPT ä¼˜åŒ–å»ºè®®**ï¼šâœ…ä¿ç•™ âš ï¸æ”¹è¿› âŒåˆ å‡

è¯·ç”¨é¼“åŠ±ã€å®ç”¨çš„è¯­æ°”å›å¤ã€‚"""

    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_message},
                    image_content
                ]
            }
        ],
        max_tokens=2000,
        temperature=0.7
    )

    return response.choices[0].message.content


# åˆ›å»ºå…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹
llm_client = LLMClient()
